{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastapi as fast\n",
    "from database_conntecion_miware import connect_to_postgresql\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "import psycopg2\n",
    "import logging\n",
    "from psycopg2.extras import execute_values\n",
    "from fastapi import FastAPI, HTTPException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading CSV file: C:\\Users\\DELL\\Desktop\\MI-ware\\Development\\seed_data\\customer_transactions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to CockroachDB\n",
      "INFO:__main__:Table customer_transaction is ready\n",
      "INFO:__main__:Total rows: 75\n",
      "INFO:__main__:Batch size (30%): 22 rows\n",
      "INFO:__main__:Number of batches: 4\n",
      "INFO:__main__:Batch 1/4 inserted successfully (0 to 22 rows)\n",
      "INFO:__main__:Batch 2/4 inserted successfully (22 to 44 rows)\n",
      "INFO:__main__:Batch 3/4 inserted successfully (44 to 66 rows)\n",
      "INFO:__main__:Batch 4/4 inserted successfully (66 to 75 rows)\n",
      "INFO:__main__:Process completed successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    filename=\"app.log\",  # Save logs to a file\n",
    "    filemode=\"w\"         # Overwrite the file each time (use \"a\" for appending)\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def create_table_if_not_exists(conn, df: pd.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"Create table if it doesn't exist using pure SQL.\"\"\"\n",
    "    columns = [f\"{col} TEXT\" for col in df.columns]\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        {', '.join(columns)}\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.execute(create_table_sql)\n",
    "            conn.commit()\n",
    "            logger.info(f\"Table {table_name} is ready\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error creating table: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def insert_data_in_batches(conn, df: pd.DataFrame, table_name: str, batch_percentage: int) -> None:\n",
    "    \"\"\"Insert data using psycopg2 execute_values in percentage-based batches.\"\"\"\n",
    "    # Convert all data to strings\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    # Calculate batch size based on percentage\n",
    "    total_rows = len(df)\n",
    "    batch_size = int(total_rows * (batch_percentage / 100))\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size  # Round up division\n",
    "    \n",
    "    logger.info(f\"Total rows: {total_rows}\")\n",
    "    logger.info(f\"Batch size ({batch_percentage}%): {batch_size} rows\")\n",
    "    logger.info(f\"Number of batches: {num_batches}\")\n",
    "    \n",
    "    # Prepare the insert SQL\n",
    "    columns = df.columns.tolist()\n",
    "    insert_sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        for batch_num in range(num_batches):\n",
    "            try:\n",
    "                # Calculate start and end indices for this batch\n",
    "                start_idx = batch_num * batch_size\n",
    "                end_idx = min((batch_num + 1) * batch_size, total_rows)\n",
    "                \n",
    "                # Get the batch data\n",
    "                batch_df = df.iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Convert batch to list of tuples\n",
    "                batch_values = [tuple(x) for x in batch_df.values]\n",
    "                \n",
    "                # Insert batch\n",
    "                execute_values(cur, insert_sql, batch_values)\n",
    "                conn.commit()\n",
    "                \n",
    "                logger.info(f\"Batch {batch_num + 1}/{num_batches} inserted successfully ({start_idx} to {end_idx} rows)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                logger.error(f\"Error inserting batch {batch_num + 1}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from CSV or Excel files.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "    try:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            data = pd.read_csv(filename)\n",
    "            logger.info(f\"Reading CSV file: {filename}\")\n",
    "        elif filename.endswith(\".xlsx\"):\n",
    "            data = pd.read_excel(filename)\n",
    "            logger.info(f\"Reading Excel file: {filename}\")\n",
    "        elif filename.endswith(\".xls\"):\n",
    "            data = pd.read_excel(filename, engine=\"xlrd\")\n",
    "            logger.info(f\"Reading legacy Excel file: {filename}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format!\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {filename}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_excel_to_postgres(filename: str, table_name: str, database_url: str) -> None:\n",
    "    \"\"\"Main function to process file and load it into CockroachDB.\"\"\"\n",
    "    try:\n",
    "        # Read the file\n",
    "        df = read_data(filename)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"The file is empty\")\n",
    "            \n",
    "        # Connect to database\n",
    "        conn = connect_to_postgresql(database_url)\n",
    "        \n",
    "        # Create table if needed\n",
    "        create_table_if_not_exists(conn, df, table_name)\n",
    "        \n",
    "        # Insert data\n",
    "        insert_data_in_batches(conn, df, table_name,30)\n",
    "        \n",
    "        # Close connection\n",
    "        conn.close()\n",
    "        logger.info(\"Process completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Get database credentials\n",
    "        Username = os.getenv(\"Username_database\")\n",
    "        Host = os.getenv(\"Host\")\n",
    "        Port = os.getenv(\"Port\")\n",
    "        password = os.getenv(\"Password\")\n",
    "        Database = os.getenv(\"Database\")\n",
    "        \n",
    "        if not all([Username, Host, Port, password, Database]):\n",
    "            raise ValueError(\"Missing required environment variables\")\n",
    "            \n",
    "        # Construct database URL\n",
    "        database_url = f\"postgresql://{Username}:{password}@{Host}:{Port}/{Database}?sslmode=require\"\n",
    "        \n",
    "        # Process file\n",
    "        process_excel_to_postgres(\n",
    "            filename=r\"C:\\Users\\DELL\\Desktop\\MI-ware\\Development\\seed_data\\customer_transactions.csv\",\n",
    "            table_name=\"customer_transaction\",\n",
    "            database_url=database_url\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    filemode=\"w\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(title=\"Excel to CockroachDB Loader\")\n",
    "\n",
    "class DatabaseParams(BaseModel):\n",
    "    username: str\n",
    "    password: str\n",
    "    host: str\n",
    "    port: int\n",
    "    database: str\n",
    "    table_name: str\n",
    "    batch_percentage: int = 30\n",
    "\n",
    "def connect_to_postgresql(database_url: str):\n",
    "    \"\"\"Create database connection using psycopg2.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(database_url)\n",
    "        logger.info(\"Successfully connected to CockroachDB\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Connection error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_table_if_not_exists(conn, df: pd.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"Create table if it doesn't exist using pure SQL.\"\"\"\n",
    "    columns = [f\"{col} TEXT\" for col in df.columns]\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        {', '.join(columns)}\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.execute(create_table_sql)\n",
    "            conn.commit()\n",
    "            logger.info(f\"Table {table_name} is ready\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error creating table: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def insert_data_in_batches(conn, df: pd.DataFrame, table_name: str, batch_percentage: int) -> dict:\n",
    "    \"\"\"Insert data using psycopg2 execute_values in percentage-based batches.\"\"\"\n",
    "    status_updates = []\n",
    "    \n",
    "    # Convert all data to strings\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    # Calculate batch size based on percentage\n",
    "    total_rows = len(df)\n",
    "    batch_size = int(total_rows * (batch_percentage / 100))\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    logger.info(f\"Total rows: {total_rows}\")\n",
    "    logger.info(f\"Batch size ({batch_percentage}%): {batch_size} rows\")\n",
    "    logger.info(f\"Number of batches: {num_batches}\")\n",
    "    \n",
    "    # Prepare the insert SQL\n",
    "    columns = df.columns.tolist()\n",
    "    insert_sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        for batch_num in range(num_batches):\n",
    "            try:\n",
    "                start_idx = batch_num * batch_size\n",
    "                end_idx = min((batch_num + 1) * batch_size, total_rows)\n",
    "                batch_df = df.iloc[start_idx:end_idx]\n",
    "                batch_values = [tuple(x) for x in batch_df.values]\n",
    "                \n",
    "                execute_values(cur, insert_sql, batch_values)\n",
    "                conn.commit()\n",
    "                \n",
    "                status_message = f\"Batch {batch_num + 1}/{num_batches} inserted ({start_idx} to {end_idx} rows)\"\n",
    "                logger.info(status_message)\n",
    "                status_updates.append(status_message)\n",
    "                \n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                error_msg = f\"Error inserting batch {batch_num + 1}: {str(e)}\"\n",
    "                logger.error(error_msg)\n",
    "                raise HTTPException(status_code=500, detail=error_msg)\n",
    "    \n",
    "    return {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"batches_completed\": num_batches,\n",
    "        \"status_updates\": status_updates\n",
    "    }\n",
    "\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_file(\n",
    "    file: UploadFile = File(...),\n",
    "    db_params: DatabaseParams = Form(...)\n",
    "):\n",
    "    try:\n",
    "        # Save uploaded file temporarily\n",
    "        temp_file_path = f\"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}{os.path.splitext(file.filename)[1]}\"\n",
    "        with open(temp_file_path, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "        \n",
    "        # Construct database URL\n",
    "        database_url = f\"postgresql://{db_params.username}:{db_params.password}@{db_params.host}:{db_params.port}/{db_params.database}?sslmode=require\"\n",
    "        \n",
    "        # Read the file\n",
    "        df = pd.read_csv(temp_file_path) if temp_file_path.endswith('.csv') else pd.read_excel(temp_file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise HTTPException(status_code=400, detail=\"The uploaded file is empty\")\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = connect_to_postgresql(database_url)\n",
    "        \n",
    "        # Create table if needed\n",
    "        create_table_if_not_exists(conn, df, db_params.table_name)\n",
    "        \n",
    "        # Insert data and get status\n",
    "        result = insert_data_in_batches(conn, df, db_params.table_name, db_params.batch_percentage)\n",
    "        \n",
    "        # Clean up\n",
    "        conn.close()\n",
    "        os.remove(temp_file_path)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Data processed successfully\",\n",
    "            \"details\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Clean up temp file if it exists\n",
    "        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "            \n",
    "        logger.error(f\"Error processing upload: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
